{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vježba 3 - Vektorska reprezentacija dokumenta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol><li> Kreirati promjenljivu <b>str1</b> koja sadrži tekst 'zdravo', te listu <b>str_list</b> koja sadrži stringove: 'zdravo', 'zbogom', 'zdravo', 'zbogom', 'zdravo', 'zdravoratumski'. Napisati kod koji broji koliko puta se string 'zdravo' nalazi u listi <b>str_list</b>.\n",
    "</li></ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "#KOD ZA ZADATAK 1\n",
    "\n",
    "str1 = 'zdravo'\n",
    "str_list = ['zdravo', 'zbogom', 'zdravo', 'zbogom', 'zdravo', 'zdravoratumski']\n",
    "counter = str_list.count(str1)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol start = \"2\">\n",
    "<li> Napisati kod koji se u listi <b>str_list</b> pronalazi string najsličniji riječi 'zdravorazumski', pri čemu se sličnost mjeri brojem istih slova.\n",
    "</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zdravoratumski\n"
     ]
    }
   ],
   "source": [
    "#KOD ZA ZADATAK 2\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def sim(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "find = 'zdravorazumski'\n",
    "best_res = 0\n",
    "best_match = ''\n",
    "for word in str_list:\n",
    "    res = sim(word, find)\n",
    "    if res > best_res :\n",
    "        best_res = res\n",
    "        best_match = word\n",
    "        \n",
    "print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol start = \"3\">\n",
    "<li> Kreirajte promjenljivu koja sadrži string 'eci peci pec ti si mali zec'. Tokenizujte ovu promjenljivu na riječi koje sadrži. Rezultat treba da bude lista koja sadrži pojedine riječi zadatog stringa.\n",
    "</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eci', 'peci', 'pec', 'ti', 'si', 'mali', 'zec']\n"
     ]
    }
   ],
   "source": [
    "#KOD ZA ZADATAK 3\n",
    "sentence =  'eci peci pec ti si mali zec'\n",
    "words = sentence.split(' ')\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "U narednom dijelu vježbe ćemo iskoristiti korpus dokumenata sa projekta Gutenberg da bismo kreirali njihovu vektorsku reprezentaciju, a nakon toga implementirali sistem koji za proizvoljan dokument pronalazi najsličniji dokument iz Gutenberg korpusa. U narednoj ćeliji koda su date dvije funkcije koje se mogu iskoristiti za tokenizaciju dokumenta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "minlen = 1\n",
    "\n",
    "def normalize(word):\n",
    "    # Funkcija koja vrši normalizaciju riječi\n",
    "    word = re.sub('[^A-Za-z0-9]+', '', word)    #Ukljanjanje svih karakter koji nisu alfanumerički\n",
    "    word = re.sub('[0-9]+', 'number', word)     #Zamjena svih brojeva u tekstu riječju number\n",
    "    word = word.lower()                         #Pretvaranje u mala slova\n",
    "    return word\n",
    "\n",
    "def tokenize(text):\n",
    "    #Funkcija koja vrši tokenizaciju teksta na sastavne riječi\n",
    "    tokens = nltk.word_tokenize(text)           #Tokenizacija teksta\n",
    "    stems = []\n",
    "    for token in tokens:\n",
    "        word = normalize(token)                 #Normalizacija pronađenih tokena\n",
    "        stem = stemmer.stem(word)               #Stemizacija tokena\n",
    "        if len(stem) > minlen:                  #Zadržavanje tokena čija je dužina veća od minlen\n",
    "            stems.append(stem)\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potrebno je napisati kod koji će za sve dokumente iz Gutenberg korpusa izračunati njihovu vektorsku reprezentaciju i to sačuvati u promjenljivu representations. Da bi se to uradilo potrebno je:\n",
    "1. Za svaki dokument odrediti tokene,\n",
    "2. Odrediti rječnik za dati korpus,\n",
    "3. Za svaki dokument na osnovu rječnika odrediti vektorsku reprezentaciju.\n",
    "U narednoj ćeliji su date kostur potrebnog koda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Putanja do korpusa\n",
    "path = './gutenberg'\n",
    "\n",
    "#Dictionary koji će za svaki dokument sadržati njegove tokene\n",
    "token_dict = {}\n",
    "\n",
    "#Prolazak kroz sve fajlove\n",
    "for dirpath, dirs, files in os.walk(path):\n",
    "    for f in files:\n",
    "        fname = os.path.join(dirpath, f)    #Putanja do fajla\n",
    "        #Izršiti učitavanje fajla i njegovu tokenizaciju\n",
    "        #Tokene svakog fajla upisati u token_dict i to tako da je ključ naziv fajla, a vrijednost tokeni\n",
    "        #KOD\n",
    "        with open(fname, 'r') as f:\n",
    "            text = f.read()\n",
    "            tokens = tokenize(text)\n",
    "            token_dict[fname] = tokens\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29670\n"
     ]
    }
   ],
   "source": [
    "#Lista koja će predstavljati rječnik svih rječi iz korpusa\n",
    "dictionary = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#Proći kroz tokene za sve fajlove i dodati ih u dictionary, obratiti pažnju da često korištene riječi (stop_words) nije potrebno\n",
    "#dodavati u rječnik\n",
    "\n",
    "#KOD\n",
    "for tokens in token_dict.values():\n",
    "    for word in tokens:\n",
    "        if word not in stop_words: dictionary.append(word)\n",
    "            \n",
    "#Kako rječnik ne bi trebalo da sadrži ponovljene riječi potrebno je ostaviti samo jedinstvene riječi\n",
    "\n",
    "#KOD\n",
    "\n",
    "dictionary = set(dictionary)\n",
    "\n",
    "#Kako bi se ubrzalo pretraživanje rječnika, rječnik se pretvara u dictionary koji za ključeve ima riječi, a za vrijednosti\n",
    "#indeks koji odgovara poziciji te riječi u reprezentaciji\n",
    "\n",
    "dictionary_dict = {word: i for i, word in enumerate(dictionary)}\n",
    "\n",
    "print(len(dictionary_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_document_representation(tokens):\n",
    "    #Napisati funkciju koja kreira reprezentaciju dokumenta na osnovu ulaznih tokena i to tako da se za svaku riječ iz rječnika\n",
    "    #odredi koliko puta se pojavljuje u dokumentu\n",
    "    #Izlaz treba biti niz ranga 1 dužine koja odgovara dužini rječnika\n",
    "    \n",
    "    #KOD\n",
    "    rep = np.zeros(len(dictionary_dict))\n",
    "    for token in tokens:\n",
    "        if token not in stop_words and token in dictionary_dict.keys():\n",
    "            #print(token)\n",
    "            rep[dictionary_dict[token]] += 1\n",
    "    return rep\n",
    "    \n",
    "#Kreiranje numpy niza koji će sadržati reprezentacije svakog dokumenta iz korpusa, shape je (broj_dokumenata, broj_rječi)\n",
    "representations = np.zeros((len(token_dict), len(dictionary_dict)))\n",
    "\n",
    "#Kreirati reprezentaciju svih dokumenta iz korpusa tako što ćete iskoristiti prethodno napisanu funkciju\n",
    "\n",
    "#KOD\n",
    "for i, tokens in enumerate(token_dict.values()):\n",
    "    representations[i] = compute_document_representation(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nakon što smo odredili reprezentaciju za sve dokumente ove reprezentacije je moguće iskoristiti za pretraživanje. Pretraživanje izvršiti na način da se za proizvoljno izabran dokument pronalaze tri najsličnija dokumenta iz korpusa. Za računanje sličnosti iskoristiti i SSD i kosinusnu sličnost. Za računanje kosinusne sličnosti je moguće iskoristiti funkciju cosine_similarity iz modula sklearn.metrics.pairwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./gutenberg\\austen-emma.txt\n",
      "0\n",
      "./gutenberg\\austen-persuasion.txt\n",
      "1\n",
      "./gutenberg\\austen-sense.txt\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import spatial\n",
    "\n",
    "#KOD\n",
    "\n",
    "def ssd(a, b):\n",
    "    return ((a - b)**2).sum()\n",
    "\n",
    "def cos_sim(a, b):\n",
    "    return spatial.distance.cosine(a, b)\n",
    "\n",
    "def findSimilar(index):\n",
    "    main_rep = representations[index]\n",
    "    sim_list = []\n",
    "    for rep in representations:\n",
    "        ssd_val = cos_sim(main_rep, rep)\n",
    "        sim_list.append(ssd_val)\n",
    "    \n",
    "    return np.argsort(sim_list)[0:3]\n",
    "\n",
    "result = findSimilar(0)\n",
    "\n",
    "for num in result:\n",
    "    for n, name in enumerate(token_dict.keys()):\n",
    "        if n == num: \n",
    "            print(name)\n",
    "            print(n)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Za računanje TFIDF reprezentacije dokumenta moguće je iskoristiti sklearn biblioteku i kod dat u narednoj ćeliji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./gutenberg\\austen-emma.txt\n",
      "./gutenberg\\austen-persuasion.txt\n",
      "./gutenberg\\austen-sense.txt\n",
      "./gutenberg\\bible-kjv.txt\n",
      "./gutenberg\\blake-poems.txt\n",
      "./gutenberg\\bryant-stories.txt\n",
      "./gutenberg\\burgess-busterbrown.txt\n",
      "./gutenberg\\carroll-alice.txt\n",
      "./gutenberg\\chesterton-ball.txt\n",
      "./gutenberg\\chesterton-brown.txt\n",
      "./gutenberg\\chesterton-thursday.txt\n",
      "./gutenberg\\edgeworth-parents.txt\n",
      "./gutenberg\\melville-moby_dick.txt\n",
      "./gutenberg\\milton-paradise.txt\n",
      "./gutenberg\\shakespeare-caesar.txt\n",
      "./gutenberg\\shakespeare-hamlet.txt\n",
      "./gutenberg\\shakespeare-macbeth.txt\n",
      "./gutenberg\\whitman-leaves.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jova\\Anaconda3\\envs\\mms\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:286: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<1x29559 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 4 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "\n",
    "#Putanja do korpusa\n",
    "path = './gutenberg'\n",
    "\n",
    "#Dictionary koji sadrži tekst svakog dokumenta\n",
    "text_dict = {}\n",
    "\n",
    "#Čitanje sadržaja svih dokumenata\n",
    "for dirpath, dirs, files in os.walk(path):\n",
    "    for f in files:\n",
    "        fname = os.path.join(dirpath, f)\n",
    "        print (fname)\n",
    "        with open(fname) as file:\n",
    "            text_dict[f] = file.read()\n",
    "\n",
    "#Definisanje objekta koji služi za određivanje TFIDF reprezentacije, za tokenizaciju se koristifunkcije data\n",
    "#kao parametar tokenize, dok se kao stop riječi koriste one iz enegleskog jezika (moguće je zadati i listu koja sadrži stop riječi)\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words='english')\n",
    "\n",
    "#Učenje rječnika i transformacija dokumenata koji su iskorišteni za učenje rječnika\n",
    "#U representations_tfidf se sada nalazi rezrezentacija svih dokumenata iz korpusa\n",
    "representations_tfidf = tfidf.fit_transform(text_dict.values())\n",
    "\n",
    "str = 'all great and precious things are lonely.'\n",
    "\n",
    "#Određivanje reprezentacije za novi dokument\n",
    "#Treba obratiti pažnju da su reprezentacije date kao sparse matrice radi efikasnijg čuvanja u memoriji ali i bržeg procesiranja.\n",
    "response = tfidf.transform([str])\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Za proizvoljan dokument odrediti 3 najsličnija dokumenta iz korpusa na osnovu SSD i kosinusne sličnosti. Prethodno navedenu funkciju za kosinusnu sličnost je moguće koristiti i sa sparse matricama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "austen-persuasion.txt\n",
      "edgeworth-parents.txt\n",
      "austen-emma.txt\n"
     ]
    }
   ],
   "source": [
    "#KOD\n",
    "\n",
    "doc = representations_tfidf[0]\n",
    "kernel_matrix = cosine_similarity(doc, representations_tfidf)\n",
    "\n",
    "list = np.argsort(kernel_matrix)\n",
    "\n",
    "index = list[0][-3:]\n",
    "\n",
    "for num in index:\n",
    "    for n, name in enumerate(text_dict.keys()):\n",
    "        if n == num: print(name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
